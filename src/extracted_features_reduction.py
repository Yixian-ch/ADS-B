import os
import gc
import numpy as np
import pandas as pd
from glob import glob
from tqdm import tqdm
import argparse
import json

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import IncrementalPCA

# è®¾ç½®æ—¥å¿—
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("GroupedReduction")

def classify_features_intelligent(feature_names):
    """
    æ ¹æ®ç‰¹å¾åç§°æ™ºèƒ½åˆ†ç±»ä¸ºç¬æ€å’Œç¨³æ€ç‰¹å¾
    """
    # ç¬æ€ç‰¹å¾å…³é”®è¯ - ä¸ä¿¡å·å˜åŒ–ã€åŠ¨æ€ç‰¹æ€§ç›¸å…³
    transient_keywords = [
        'variation', 'std', 'max', 'min', 'range', 'dynamic',
        'detail', 'peak', 'envelope_variation', 'phase_std',
        'freq_std', 'freq_max', 'freq_min', 'freq_range',
        'modulation_index', 'spectral_std', 'spectral_max',
        'wavelet_detail', 'euclidean_dist', 'dtw_dist',
        'spectral_skewness', 'spectral_kurtosis', 'spectral_crest'
    ]
    
    # ç¨³æ€ç‰¹å¾å…³é”®è¯ - ä¸å¹³å‡å€¼ã€ç¨³å®šçŠ¶æ€ç›¸å…³
    steady_keywords = [
        'mean', 'approx', 'correlation', 'energy', 'entropy',
        'envelope_mean', 'freq_mean', 'phase_mean', 'spectral_mean',
        'wavelet_approx', 'bandwidth', 'odd_even_ratio',
        'mean_freq_offset', 'envelope_spectral', 'freq_spectral'
    ]
    
    transient_features = []
    steady_features = []
    
    for feature in feature_names:
        feature_lower = feature.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¬æ€å…³é”®è¯
        is_transient = any(keyword in feature_lower for keyword in transient_keywords)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¨³æ€å…³é”®è¯
        is_steady = any(keyword in feature_lower for keyword in steady_keywords)
        
        if is_transient and not is_steady:
            transient_features.append(feature)
        elif is_steady and not is_transient:
            steady_features.append(feature)
        elif is_transient and is_steady:
            # å¦‚æœåŒæ—¶åŒ…å«ä¸¤ç§å…³é”®è¯ï¼Œä¼˜å…ˆè€ƒè™‘æ›´æ˜æ˜¾çš„ç‰¹å¾
            if any(kw in feature_lower for kw in ['std', 'variation', 'max', 'min', 'range']):
                transient_features.append(feature)
            else:
                steady_features.append(feature)
        else:
            # é»˜è®¤åˆ†ä¸ºç¨³æ€ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            steady_features.append(feature)
    
    logger.info(f"ç¬æ€ç‰¹å¾æ•°é‡: {len(transient_features)}")
    logger.info(f"ç¨³æ€ç‰¹å¾æ•°é‡: {len(steady_features)}")
    logger.info(f"ç¬æ€ç‰¹å¾ç¤ºä¾‹: {transient_features[:5]}")
    logger.info(f"ç¨³æ€ç‰¹å¾ç¤ºä¾‹: {steady_features[:5]}")
    
    return transient_features, steady_features

def integrate_features_chunked(enhanced_data_path, chunk_size=1000, output_file=None):
    """æ•´åˆç‰¹å¾æ•°æ®"""
    logger.info(f"å¼€å§‹æ•´åˆç‰¹å¾æ•°æ®")
    
    routes = ['I', 'Q']
    if output_file is None:
        output_file = os.path.join(os.path.dirname(enhanced_data_path), "integrated_features.csv")
    
    all_feature_files = []
    
    # æ”¶é›†æ‰€æœ‰ç‰¹å¾æ–‡ä»¶
    for route in routes:
        route_path = os.path.join(enhanced_data_path, f"{route}_data")
        if not os.path.exists(route_path):
            continue
        
        individual_dirs = [d for d in os.listdir(route_path) 
                          if os.path.isdir(os.path.join(route_path, d))]
        
        for individual_id in individual_dirs:
            features_file = os.path.join(route_path, individual_id, "enhanced_features.csv")
            if os.path.exists(features_file):
                all_feature_files.append((features_file, individual_id, route))
    
    logger.info(f"æ‰¾åˆ° {len(all_feature_files)} ä¸ªç‰¹å¾æ–‡ä»¶")
    
    # æ•´åˆæ•°æ®
    first_chunk = True
    total_samples = 0
    
    for i in tqdm(range(0, len(all_feature_files), chunk_size), desc="æ•´åˆç‰¹å¾"):
        chunk_files = all_feature_files[i:i+chunk_size]
        chunk_dfs = []
        
        for file_path, individual_id, route in chunk_files:
            try:
                df = pd.read_csv(file_path)
                chunk_dfs.append(df)
            except Exception as e:
                logger.warning(f"è¯»å–å¤±è´¥: {file_path}")
        
        if chunk_dfs:
            chunk_df = pd.concat(chunk_dfs, ignore_index=True)
            
            if first_chunk:
                chunk_df.to_csv(output_file, index=False, mode='w')
                first_chunk = False
            else:
                chunk_df.to_csv(output_file, index=False, mode='a', header=False)
            
            total_samples += len(chunk_df)
            del chunk_df, chunk_dfs
            gc.collect()
    
    logger.info(f"æ•´åˆå®Œæˆ: {total_samples} ä¸ªæ ·æœ¬")
    return total_samples, output_file

def preprocess_features_grouped(input_file, chunk_size=5000):
    """é¢„å¤„ç†å¹¶åˆ†ç»„ç‰¹å¾"""
    logger.info("å¼€å§‹é¢„å¤„ç†å’Œåˆ†ç»„ç‰¹å¾")
    
    # è¯»å–æ•°æ®è·å–ç‰¹å¾ä¿¡æ¯
    first_chunk = pd.read_csv(input_file, nrows=1000)
    
    # è¯†åˆ«æ•°å€¼ç‰¹å¾ï¼ˆæ’é™¤IDå’Œå…ƒæ•°æ®åˆ—ï¼‰
    exclude_cols = ['individual_id', 'signal_idx', 'route']
    all_feature_cols = [col for col in first_chunk.columns 
                       if col not in exclude_cols and 
                       first_chunk[col].dtype in ['int64', 'float64', 'int32', 'float32']]
    
    logger.info(f"æ€»ç‰¹å¾æ•°: {len(all_feature_cols)}")
    
    # æ™ºèƒ½åˆ†ç»„ç‰¹å¾
    transient_features, steady_features = classify_features_intelligent(all_feature_cols)
    
    # ğŸ”§ ä¿®å¤ç›®æ ‡ç¼–ç  - ä¸€æ¬¡æ€§æ”¶é›†æ‰€æœ‰ä¿¡æ¯
    logger.info("æ”¶é›†æ•°æ®ä¿¡æ¯...")
    all_individual_ids = set()
    total_samples = 0
    
    # ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ•°æ®æ¥æ”¶é›†ä¿¡æ¯ï¼ˆå¯¹äºå°æ–‡ä»¶æ›´å¯é ï¼‰
    try:
        # å…ˆå°è¯•è¯»å–æ•´ä¸ªæ–‡ä»¶
        full_df = pd.read_csv(input_file)
        total_samples = len(full_df)
        
        if 'individual_id' in full_df.columns:
            # æ¸…ç†individual_idæ•°æ®
            valid_ids = full_df['individual_id'].dropna().astype(str).str.strip()
            valid_ids = valid_ids[valid_ids != '']  # å»é™¤ç©ºå­—ç¬¦ä¸²
            all_individual_ids = set(valid_ids)
            
            logger.info(f"æˆåŠŸè¯»å–æ•´ä¸ªæ–‡ä»¶: {total_samples} è¡Œ")
        else:
            logger.error("ç¼ºå°‘individual_idåˆ—")
            return None
            
    except Exception as e:
        logger.warning(f"æ— æ³•è¯»å–æ•´ä¸ªæ–‡ä»¶ ({e})ï¼Œæ”¹ç”¨åˆ†å—è¯»å–...")
        
        # å¦‚æœæ–‡ä»¶å¤ªå¤§ï¼Œé€€å›åˆ°åˆ†å—è¯»å–
        chunk_iter = pd.read_csv(input_file, chunksize=chunk_size)
        for chunk in tqdm(chunk_iter, desc="æ”¶é›†ç›®æ ‡æ ‡ç­¾"):
            if 'individual_id' in chunk.columns:
                valid_ids = chunk['individual_id'].dropna().astype(str).str.strip()
                valid_ids = valid_ids[valid_ids != '']
                all_individual_ids.update(valid_ids)
                total_samples += len(chunk)
            else:
                logger.error("ç¼ºå°‘individual_idåˆ—")
                return None
    
    # åˆ›å»ºå®Œæ•´çš„ç›®æ ‡ç¼–ç æ˜ å°„
    unique_individual_ids = sorted(list(all_individual_ids))
    target_mapping = {individual_id: i for i, individual_id in enumerate(unique_individual_ids)}
    
    logger.info(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    logger.info(f"å”¯ä¸€ä¸ªä½“æ•°: {len(unique_individual_ids)}")
    logger.info(f"ä¸ªä½“IDåˆ—è¡¨: {unique_individual_ids}")
    
    # éªŒè¯æ˜ å°„
    logger.info("ç›®æ ‡æ˜ å°„éªŒè¯:")
    for individual_id, target_id in list(target_mapping.items())[:10]:
        logger.info(f"  {individual_id} -> {target_id}")
    
    # åˆ›å»ºæ ‡å‡†åŒ–å™¨
    transient_scaler = StandardScaler()
    steady_scaler = StandardScaler()
    
    # æ‹Ÿåˆæ ‡å‡†åŒ–å™¨
    logger.info("æ‹Ÿåˆæ ‡å‡†åŒ–å™¨...")
    chunk_iter = pd.read_csv(input_file, chunksize=chunk_size)
    for chunk in tqdm(chunk_iter, desc="æ‹Ÿåˆæ ‡å‡†åŒ–å™¨"):
        if transient_features:
            transient_data = chunk[transient_features].fillna(0)
            transient_scaler.partial_fit(transient_data)
        
        if steady_features:
            steady_data = chunk[steady_features].fillna(0)
            steady_scaler.partial_fit(steady_data)
    
    # åº”ç”¨é¢„å¤„ç†
    processed_file = input_file.replace('.csv', '_processed_grouped.csv')
    first_chunk = True
    processed_samples = 0
    skipped_samples = 0
    
    logger.info("åº”ç”¨é¢„å¤„ç†...")
    chunk_iter = pd.read_csv(input_file, chunksize=chunk_size)
    for chunk in tqdm(chunk_iter, desc="é¢„å¤„ç†æ•°æ®"):
        # ğŸ”§ æ£€æŸ¥å¹¶å¤„ç†ç¼ºå¤±çš„individual_id
        if 'individual_id' not in chunk.columns:
            logger.error("chunkä¸­ç¼ºå°‘individual_idåˆ—")
            continue
            
        # è¿‡æ»¤æ‰individual_idä¸ºç©ºçš„è¡Œ
        valid_mask = chunk['individual_id'].notna()
        valid_chunk = chunk[valid_mask]
        
        if len(valid_chunk) == 0:
            logger.warning("å½“å‰chunkæ²¡æœ‰æœ‰æ•ˆçš„individual_id")
            continue
        
        # ğŸ”§ ç¡®ä¿æ‰€æœ‰individual_idéƒ½åœ¨æ˜ å°„ä¸­
        missing_ids = set(valid_chunk['individual_id'].unique()) - set(target_mapping.keys())
        if missing_ids:
            logger.warning(f"å‘ç°ç¼ºå¤±çš„individual_id: {missing_ids}")
            # ä¸ºç¼ºå¤±çš„IDåˆ›å»ºæ–°çš„æ˜ å°„
            for missing_id in missing_ids:
                target_mapping[missing_id] = len(target_mapping)
        
        # æ ‡å‡†åŒ–ç¬æ€ç‰¹å¾
        if transient_features:
            transient_scaled = transient_scaler.transform(valid_chunk[transient_features].fillna(0))
            transient_df = pd.DataFrame(transient_scaled, 
                                      columns=[f"transient_{col}" for col in transient_features])
        else:
            transient_df = pd.DataFrame()
        
        # æ ‡å‡†åŒ–ç¨³æ€ç‰¹å¾
        if steady_features:
            steady_scaled = steady_scaler.transform(valid_chunk[steady_features].fillna(0))
            steady_df = pd.DataFrame(steady_scaled,
                                   columns=[f"steady_{col}" for col in steady_features])
        else:
            steady_df = pd.DataFrame()
        
        # åˆå¹¶ç‰¹å¾å’Œå…ƒæ•°æ®
        result_df = pd.concat([transient_df, steady_df], axis=1)
        
        # ğŸ”§ å®‰å…¨åœ°å¤„ç†ç›®æ ‡ç¼–ç 
        try:
            result_df['target'] = [target_mapping[t] for t in valid_chunk['individual_id']]
            result_df['individual_id'] = valid_chunk['individual_id'].values
            result_df['route'] = valid_chunk['route'].values
            
            processed_samples += len(result_df)
            
        except KeyError as e:
            logger.error(f"ç›®æ ‡ç¼–ç é”™è¯¯: {e}")
            skipped_samples += len(valid_chunk)
            continue
        
        if first_chunk:
            result_df.to_csv(processed_file, index=False, mode='w')
            first_chunk = False
        else:
            result_df.to_csv(processed_file, index=False, mode='a', header=False)
        
        del result_df, transient_df, steady_df, valid_chunk
        gc.collect()
    
    logger.info(f"é¢„å¤„ç†å®Œæˆ: å¤„ç†äº†{processed_samples}ä¸ªæ ·æœ¬ï¼Œè·³è¿‡äº†{skipped_samples}ä¸ªæ ·æœ¬")
    logger.info(f"æœ€ç»ˆç›®æ ‡æ˜ å°„åŒ…å«{len(target_mapping)}ä¸ªä¸ªä½“")
    
    return (transient_scaler, steady_scaler, transient_features, steady_features, 
            target_mapping, processed_file)

def apply_grouped_pca(input_file, transient_components=50, steady_components=50, chunk_size=5000):
    """å¯¹ç¬æ€å’Œç¨³æ€ç‰¹å¾åˆ†åˆ«è¿›è¡ŒPCAé™ç»´"""
    logger.info(f"å¼€å§‹åˆ†ç»„PCAé™ç»´: ç¬æ€{transient_components}ç»´, ç¨³æ€{steady_components}ç»´")
    
    # è¯»å–æ•°æ®è·å–ç‰¹å¾ä¿¡æ¯
    first_chunk = pd.read_csv(input_file, nrows=1000)
    
    # è¯†åˆ«ç¬æ€å’Œç¨³æ€ç‰¹å¾åˆ—
    transient_cols = [col for col in first_chunk.columns if col.startswith('transient_')]
    steady_cols = [col for col in first_chunk.columns if col.startswith('steady_')]
    
    logger.info(f"ç¬æ€ç‰¹å¾åˆ—æ•°: {len(transient_cols)}")
    logger.info(f"ç¨³æ€ç‰¹å¾åˆ—æ•°: {len(steady_cols)}")
    
    # è°ƒæ•´ç»„ä»¶æ•°é‡
    actual_transient_components = min(transient_components, len(transient_cols))
    actual_steady_components = min(steady_components, len(steady_cols))
    
    # åˆå§‹åŒ–PCA
    transient_pca = IncrementalPCA(n_components=actual_transient_components) if transient_cols else None
    steady_pca = IncrementalPCA(n_components=actual_steady_components) if steady_cols else None
    
    # è®­ç»ƒPCA
    logger.info("è®­ç»ƒPCAæ¨¡å‹...")
    chunk_iter = pd.read_csv(input_file, chunksize=chunk_size)
    for chunk in tqdm(chunk_iter, desc="è®­ç»ƒPCA"):
        if transient_pca and transient_cols:
            transient_data = chunk[transient_cols].values
            transient_pca.partial_fit(transient_data)
        
        if steady_pca and steady_cols:
            steady_data = chunk[steady_cols].values
            steady_pca.partial_fit(steady_data)
        
        gc.collect()
    
    # åº”ç”¨PCAå˜æ¢
    output_file = input_file.replace('.csv', f'_pca_t{actual_transient_components}_s{actual_steady_components}.csv')
    first_chunk = True
    
    logger.info("åº”ç”¨PCAå˜æ¢...")
    chunk_iter = pd.read_csv(input_file, chunksize=chunk_size)
    for chunk in tqdm(chunk_iter, desc="PCAå˜æ¢"):
        result_dfs = []
        
        # ç¬æ€PCAå˜æ¢
        if transient_pca and transient_cols:
            transient_transformed = transient_pca.transform(chunk[transient_cols].values)
            transient_df = pd.DataFrame(transient_transformed,
                                      columns=[f"transient_component_{i+1}" 
                                             for i in range(actual_transient_components)])
            result_dfs.append(transient_df)
        
        # ç¨³æ€PCAå˜æ¢
        if steady_pca and steady_cols:
            steady_transformed = steady_pca.transform(chunk[steady_cols].values)
            steady_df = pd.DataFrame(steady_transformed,
                                   columns=[f"steady_component_{i+1}" 
                                          for i in range(actual_steady_components)])
            result_dfs.append(steady_df)
        
        # åˆå¹¶ç»“æœ
        if result_dfs:
            result_df = pd.concat(result_dfs, axis=1)
            
            # ğŸ”§ é‡ç½®ç´¢å¼•ç¡®ä¿å¯¹é½
            result_df = result_df.reset_index(drop=True)
            chunk = chunk.reset_index(drop=True)
            
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            if len(result_df) != len(chunk):
                logger.error(f"æ•°æ®é•¿åº¦ä¸ä¸€è‡´: result_df={len(result_df)}, chunk={len(chunk)}")
                continue
        else:
            result_df = pd.DataFrame()
        
        # æ·»åŠ å…ƒæ•°æ® - ğŸ”§ ç¡®ä¿æ•°æ®å¯¹é½
        try:
            result_df['target'] = chunk['target'].values
            result_df['individual_id'] = chunk['individual_id'].values  
            result_df['route'] = chunk['route'].values
            
            # éªŒè¯æ²¡æœ‰NaN
            if result_df['target'].isna().any():
                logger.error(f"å‘ç°target NaNå€¼ï¼Œæ•°é‡: {result_df['target'].isna().sum()}")
                continue
                
        except Exception as e:
            logger.error(f"å…ƒæ•°æ®æ·»åŠ å¤±è´¥: {e}")
            continue
        
        if first_chunk:
            result_df.to_csv(output_file, index=False, mode='w')
            first_chunk = False
        else:
            result_df.to_csv(output_file, index=False, mode='a', header=False)
        
        del result_df
        gc.collect()
    
    logger.info(f"PCAå®Œæˆï¼Œæ€»ç»´åº¦: {actual_transient_components + actual_steady_components}")
    
    return transient_pca, steady_pca, output_file

def verify_output_data(output_file):
    """éªŒè¯è¾“å‡ºæ•°æ®çš„å®Œæ•´æ€§"""
    logger.info("éªŒè¯è¾“å‡ºæ•°æ®å®Œæ•´æ€§...")
    
    try:
        # è¯»å–è¾“å‡ºæ–‡ä»¶
        df = pd.read_csv(output_file, low_memory=False)
        logger.info(f"è¾“å‡ºæ–‡ä»¶å½¢çŠ¶: {df.shape}")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_cols = ['target', 'individual_id', 'route']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.error(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return False
        
        # æ£€æŸ¥NaNå€¼
        nan_counts = df[required_cols].isnull().sum()
        for col, count in nan_counts.items():
            if count > 0:
                logger.error(f"{col}åˆ—æœ‰{count}ä¸ªNaNå€¼")
                logger.error(f"NaNè¡Œç¤ºä¾‹: {df[df[col].isnull()].index.tolist()[:10]}")
                return False
        
        # æ£€æŸ¥ç©ºå­—ç¬¦ä¸²
        for col in ['individual_id', 'route']:
            empty_count = (df[col].astype(str).str.strip() == '').sum()
            if empty_count > 0:
                logger.error(f"{col}åˆ—æœ‰{empty_count}ä¸ªç©ºå­—ç¬¦ä¸²")
                return False
        
        # ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"å”¯ä¸€individual_idæ•°é‡: {df['individual_id'].nunique()}")
        logger.info(f"å”¯ä¸€targetæ•°é‡: {df['target'].nunique()}")
        logger.info(f"routeåˆ†å¸ƒ: {df['route'].value_counts().to_dict()}")
        logger.info(f"targetèŒƒå›´: {df['target'].min()} - {df['target'].max()}")
        
        # æ˜¾ç¤ºå‰å‡ è¡Œ
        logger.info("å‰5è¡Œæ•°æ®:")
        for i in range(min(5, len(df))):
            row = df.iloc[i]
            logger.info(f"  è¡Œ{i}: target={row['target']}, id={row['individual_id']}, route={row['route']}")
        
        logger.info("âœ… æ•°æ®éªŒè¯é€šè¿‡") 
        return True
        
    except Exception as e:
        logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
        return False
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    for file_path in file_paths:
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
            except:
                pass

def cleanup_output_data(output_file):
    """æ¸…ç†è¾“å‡ºæ•°æ®ä¸­çš„é—®é¢˜è¡Œ"""
    logger.info("æ¸…ç†è¾“å‡ºæ•°æ®...")
    
    try:
        # è¯»å–æ•°æ®
        df = pd.read_csv(output_file, low_memory=False)
        original_len = len(df)
        logger.info(f"åŸå§‹æ•°æ®é•¿åº¦: {original_len}")
        
        # åˆ é™¤individual_idæˆ–routeä¸ºç©ºçš„è¡Œ
        df = df.dropna(subset=['individual_id', 'route', 'target'])
        logger.info(f"åˆ é™¤NaNåé•¿åº¦: {len(df)}")
        
        # åˆ é™¤individual_idæˆ–routeä¸ºç©ºå­—ç¬¦ä¸²çš„è¡Œ
        df = df[(df['individual_id'].astype(str).str.strip() != '') & 
                (df['route'].astype(str).str.strip() != '')]
        logger.info(f"åˆ é™¤ç©ºå­—ç¬¦ä¸²åé•¿åº¦: {len(df)}")
        
        # ç¡®ä¿targetæ˜¯æ•´æ•°
        df['target'] = df['target'].astype(int)
        
        # é‡æ–°ä¿å­˜
        df.to_csv(output_file, index=False)
        logger.info(f"âœ… æ•°æ®æ¸…ç†å®Œæˆï¼Œæœ€ç»ˆé•¿åº¦: {len(df)}")
        
        return len(df) > 0
        
    except Exception as e:
        logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {e}")
        return False

def cleanup_temp_files(*file_paths):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    for file_path in file_paths:
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
                logger.info(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ™ºèƒ½åˆ†ç»„ç‰¹å¾é™ç»´")
    
    parser.add_argument("--data_path", type=str, required=True, help="å¢å¼ºæ•°æ®è·¯å¾„")
    parser.add_argument("--output_path", type=str, default="./grouped_results", help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--transient_components", type=int, default=15, help="ç¬æ€ç‰¹å¾PCAç»„ä»¶æ•°")
    parser.add_argument("--steady_components", type=int, default=15, help="ç¨³æ€ç‰¹å¾PCAç»„ä»¶æ•°")
    parser.add_argument("--chunk_size", type=int, default=2000, help="åˆ†å—å¤§å°")
    parser.add_argument("--keep_temp_files", action="store_true", help="ä¿ç•™ä¸´æ—¶æ–‡ä»¶")
    
    args = parser.parse_args()
    
    os.makedirs(args.output_path, exist_ok=True)
    
    logger.info("å¼€å§‹æ™ºèƒ½åˆ†ç»„ç‰¹å¾é™ç»´")
    logger.info(f"ç¬æ€ç»„ä»¶: {args.transient_components}, ç¨³æ€ç»„ä»¶: {args.steady_components}")
    
    temp_files = []
    
    try:
        # 1. æ•´åˆç‰¹å¾
        logger.info("=== æ­¥éª¤1: æ•´åˆç‰¹å¾ ===")
        total_samples, integrated_file = integrate_features_chunked(
            args.data_path, chunk_size=args.chunk_size
        )
        temp_files.append(integrated_file)
        
        # 2. é¢„å¤„ç†å’Œåˆ†ç»„
        logger.info("=== æ­¥éª¤2: é¢„å¤„ç†å’Œæ™ºèƒ½åˆ†ç»„ ===")
        (transient_scaler, steady_scaler, transient_features, steady_features,
         target_mapping, processed_file) = preprocess_features_grouped(
            integrated_file, chunk_size=args.chunk_size
        )
        temp_files.append(processed_file)
        
        # 3. åˆ†ç»„PCAé™ç»´
        logger.info("=== æ­¥éª¤3: åˆ†ç»„PCAé™ç»´ ===")
        transient_pca, steady_pca, final_file = apply_grouped_pca(
            processed_file,
            transient_components=args.transient_components,
            steady_components=args.steady_components,
            chunk_size=args.chunk_size
        )
        
        # 4. éªŒè¯å’Œæ¸…ç†è¾“å‡ºæ•°æ®
        logger.info("=== æ­¥éª¤4: éªŒè¯å’Œæ¸…ç†è¾“å‡ºæ•°æ® ===")
        if not verify_output_data(final_file):
            logger.warning("âš ï¸ æ•°æ®éªŒè¯å¤±è´¥ï¼Œå°è¯•æ¸…ç†...")
            if not cleanup_output_data(final_file):
                logger.error("âŒ æ•°æ®æ¸…ç†å¤±è´¥")
                return
            # é‡æ–°éªŒè¯
            if not verify_output_data(final_file):
                logger.error("âŒ æ¸…ç†åæ•°æ®ä»æœ‰é—®é¢˜")
                return
        
        # 4. ä¿å­˜æœ€ç»ˆæ–‡ä»¶
        final_output_path = os.path.join(
            args.output_path, 
            f"grouped_reduced_t{args.transient_components}_s{args.steady_components}.csv"
        )
        
        if os.path.exists(final_file):
            os.rename(final_file, final_output_path)
            
            # éªŒè¯æ–‡ä»¶
            verify_df = pd.read_csv(final_output_path, nrows=5)
            logger.info(f"âœ… æœ€ç»ˆæ–‡ä»¶: {final_output_path}")
            logger.info(f"âœ… åˆ—æ•°: {len(verify_df.columns)}")
            logger.info(f"âœ… ç‰¹å¾åˆ—: {[c for c in verify_df.columns if 'component' in c]}")
            del verify_df
        
        # 5. ä¿å­˜æ¨¡å‹å’Œæ˜ å°„
        import joblib
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        joblib.dump(transient_scaler, os.path.join(args.output_path, "transient_scaler.joblib"))
        joblib.dump(steady_scaler, os.path.join(args.output_path, "steady_scaler.joblib"))
        
        # ä¿å­˜PCAæ¨¡å‹
        if transient_pca:
            joblib.dump(transient_pca, os.path.join(args.output_path, "transient_pca.joblib"))
        if steady_pca:
            joblib.dump(steady_pca, os.path.join(args.output_path, "steady_pca.joblib"))
        
        # ä¿å­˜ç‰¹å¾æ˜ å°„
        feature_info = {
            'transient_features': transient_features,
            'steady_features': steady_features,
            'target_mapping': target_mapping,
            'transient_components': args.transient_components,
            'steady_components': args.steady_components
        }
        
        with open(os.path.join(args.output_path, "feature_info.json"), 'w') as f:
            json.dump(feature_info, f, indent=2)
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        with open(os.path.join(args.output_path, "grouped_report.txt"), 'w') as f:
            f.write("æ™ºèƒ½åˆ†ç»„ç‰¹å¾é™ç»´æŠ¥å‘Š\n")
            f.write("=" * 30 + "\n\n")
            f.write(f"æ€»æ ·æœ¬æ•°: {total_samples}\n")
            f.write(f"åŸå§‹ç¬æ€ç‰¹å¾æ•°: {len(transient_features)}\n")
            f.write(f"åŸå§‹ç¨³æ€ç‰¹å¾æ•°: {len(steady_features)}\n")
            f.write(f"é™ç»´åç¬æ€ç»´åº¦: {args.transient_components}\n")
            f.write(f"é™ç»´åç¨³æ€ç»´åº¦: {args.steady_components}\n")
            f.write(f"æ€»é™ç»´ç»´åº¦: {args.transient_components + args.steady_components}\n")
            f.write(f"ç›®æ ‡ç±»åˆ«æ•°: {len(target_mapping)}\n\n")
            
            f.write("ç¬æ€ç‰¹å¾ç¤ºä¾‹:\n")
            for feature in transient_features[:10]:
                f.write(f"  - {feature}\n")
            f.write("\nç¨³æ€ç‰¹å¾ç¤ºä¾‹:\n")
            for feature in steady_features[:10]:
                f.write(f"  - {feature}\n")
            
            if transient_pca:
                f.write(f"\nç¬æ€PCAè§£é‡Šæ–¹å·®æ¯”: {transient_pca.explained_variance_ratio_[:5].tolist()}\n")
            if steady_pca:
                f.write(f"ç¨³æ€PCAè§£é‡Šæ–¹å·®æ¯”: {steady_pca.explained_variance_ratio_[:5].tolist()}\n")
        
        logger.info("å¤„ç†å®Œæˆï¼")
        logger.info(f"æœ€ç»ˆç‰¹å¾ç»´åº¦: ç¬æ€ {args.transient_components} + ç¨³æ€ {args.steady_components} = æ€»è®¡ {args.transient_components + args.steady_components}")
        logger.info(f"ç¬æ€ç‰¹å¾æ•°: {len(transient_features)}")
        logger.info(f"ç¨³æ€ç‰¹å¾æ•°: {len(steady_features)}")
        
    except Exception as e:
        logger.error(f"å¤„ç†å‡ºé”™: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
    finally:
        if not args.keep_temp_files:
            cleanup_temp_files(*temp_files)
        gc.collect()

if __name__ == "__main__":
    main()