import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import TensorDataset, DataLoader
import os
from tqdm import tqdm
import matplotlib.pyplot as plt
import random
import math
from sklearn.manifold import TSNE

torch.manual_seed(42)
np.random.seed(42)

def load_and_preprocess_data(data_path, test_size=0.2, val_size=0.1):
    """åŠ è½½å’Œé¢„å¤„ç†åˆ†ç»„é™ç»´åçš„æ•°æ®ï¼ˆé€‚é…æ–°ç»“æ„ï¼‰"""
    print(f"Loading data from {data_path}...")
    df = pd.read_csv(data_path)
    
    # æ–°ç»“æ„ï¼šç¬æ€ç‰¹å¾+ç¨³æ€ç‰¹å¾+target+individual_id+route
    transient_cols = [f'transient_component_{i}' for i in range(1, 16)]
    steady_cols = [f'steady_component_{i}' for i in range(1, 16)]
    feature_cols = transient_cols + steady_cols
    X = df[feature_cols].values.astype(np.float32)
    
    if 'target' in df.columns:
        label_col = 'target'
    elif 'individual_id' in df.columns:
        label_col = 'individual_id'
    else:
        raise ValueError("æ‰¾ä¸åˆ°æ ‡ç­¾åˆ—")
    
    unique_labels = sorted(df[label_col].unique())
    label_map = {label: idx for idx, label in enumerate(unique_labels)}
    y = df[label_col].map(label_map).values.astype(np.int64)
    
    print(f"ç‰¹å¾ç»´åº¦: {X.shape}, æ ‡ç­¾æ•°é‡: {len(unique_labels)}")
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    X = np.clip(X, -3, 3)
    
    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)
    val_ratio = val_size / (1 - test_size)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_ratio, random_state=42, stratify=y_temp)
    
    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))
    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))
    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))
    
    return train_dataset, val_dataset, test_dataset, len(unique_labels), label_map, scaler

def add_advanced_noise_to_batch(batch_x, min_snr=-10, max_snr=20, 
                               snr_high=0.85, snr_mid=0.14, snr_low=0.01):
    """æ›´æ™ºèƒ½çš„å™ªå£°æ·»åŠ ï¼Œé‡ç‚¹è®­ç»ƒé«˜SNR"""
    batch_size = batch_x.size(0)
    noisy_x = torch.zeros_like(batch_x)
    
    # SNRåŒºé—´æƒé‡ï¼ˆæåº¦åå‘é«˜SNRï¼Œæ¨¡ä»¿build_networkæˆåŠŸç»éªŒï¼‰
    snr_weights = {
        (15, 20): snr_high,   # é«˜SNR - 85%
        (5, 15): snr_mid,     # ä¸­SNR - 14%
        (min_snr, 5): snr_low # ä½SNR - 1%
    }
    
    def add_complex_noise(signal, snr_db):
        """æ·»åŠ å¤æ‚å™ªå£°æ¨¡å‹"""
        signal_numpy = signal.detach().cpu().numpy()
        signal_power = np.mean(signal_numpy ** 2)
        noise_power = signal_power / (10 ** (snr_db / 10))
        noise = np.random.normal(0, np.sqrt(noise_power), signal_numpy.shape)
        
        # å¯¹äºä½SNRï¼Œæ·»åŠ é¢å¤–çš„å™ªå£°ç±»å‹
        if snr_db < 0:
            # è„‰å†²å™ªå£°
            impulse_mask = np.random.rand(*signal_numpy.shape) < 0.05
            impulse_noise = np.random.randn(*signal_numpy.shape) * 3
            signal_numpy[impulse_mask] += impulse_noise[impulse_mask]
        
        noisy_signal = signal_numpy + noise
        return torch.FloatTensor(noisy_signal)
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬é€‰æ‹©SNRå¹¶æ·»åŠ å™ªå£°
    actual_snrs = []
    for i in range(batch_size):
        # éšæœºé€‰æ‹©SNRèŒƒå›´
        snr_range = random.choices(list(snr_weights.keys()), weights=list(snr_weights.values()), k=1)[0]
        
        # åœ¨é€‰å®šèŒƒå›´å†…å‡åŒ€éšæœºé€‰æ‹©SNRå€¼
        snr_db = snr_range[0] + (snr_range[1] - snr_range[0]) * random.random()
        actual_snrs.append(snr_db)
        
        # æ·»åŠ å¤æ‚å™ªå£°
        noisy_x[i] = add_complex_noise(batch_x[i], snr_db)
    
    return noisy_x, actual_snrs

class BuildInspiredAAFN(nn.Module):
    """å®Œå…¨å‚è€ƒbuild_networkæˆåŠŸæ¶æ„çš„AAFN"""
    def __init__(self, input_dim, num_classes, hidden_dim=None):
        super(BuildInspiredAAFN, self).__init__()
        self.num_classes = num_classes
        
        # è‡ªåŠ¨è®¡ç®—éšè—å±‚ç»´åº¦
        if hidden_dim is None:
            # æ ¹æ®è¾“å…¥ç»´åº¦å’Œç±»åˆ«æ•°è‡ªåŠ¨è®¡ç®—åˆé€‚çš„éšè—å±‚ç»´åº¦
            hidden_dim = max(128, min(512, input_dim * 4))
        
        # è‡ªåŠ¨è®¡ç®—dropoutç‡
        dropout_rate = min(0.3, max(0.1, 1.0 / (input_dim ** 0.5)))
        
        # è‡ªåŠ¨è®¡ç®—æ³¨æ„åŠ›å¤´æ•°ï¼Œç¡®ä¿èƒ½è¢«embed_dimæ•´é™¤
        num_heads = 1
        for i in range(8, 0, -1):
            if num_classes % i == 0:
                num_heads = i
                break
        
        # è‡ªåŠ¨è®¡ç®—SNRç¼©æ”¾å› å­
        self.snr_scale = min(50.0, max(20.0, num_classes * 0.3))
        
        print(f"è‡ªåŠ¨é€‚é…å‚æ•°:")
        print(f"- éšè—å±‚ç»´åº¦: {hidden_dim}")
        print(f"- Dropoutç‡: {dropout_rate:.3f}")
        print(f"- æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
        print(f"- SNRç¼©æ”¾å› å­: {self.snr_scale:.1f}")
        
        # å®Œå…¨å‚è€ƒbuild_networkçš„ç‰¹å¾å¤„ç†æ–¹å¼
        self.feature_processor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim * 2),
            nn.BatchNorm1d(hidden_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout_rate)
        )
        
        # ç¬æ€å’Œç¨³æ€ç‰¹å¾åˆ†ç¦»ï¼ˆbuild_networkçš„æ ¸å¿ƒæ€æƒ³ï¼‰
        self.transient_extractor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout_rate)
        )
        
        self.steady_extractor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout_rate)
        )
        
        # èåˆç‰¹å¾å¤„ç†
        self.fused_extractor = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim * 2),
            nn.BatchNorm1d(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # å‚è€ƒbuild_networkçš„ä¸¤ä¸ªä¸»è¦åˆ†æ”¯ - ä½¿ç”¨æ›´å…ˆè¿›çš„æ¶æ„
        self.vision_transformer = self._build_vision_transformer_path(hidden_dim * 2, num_classes, dropout_rate)
        self.lightweight_network = self._build_lightweight_path(hidden_dim * 2, num_classes, dropout_rate)
        
        # ç®€åŒ–çš„SNRä¼°è®¡å™¨ï¼ˆå‚è€ƒbuild_networkï¼‰
        self.snr_estimator_freq = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.LeakyReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.LeakyReLU()
        )
        
        self.snr_estimator_energy = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU()
        )
        
        self.snr_fusion = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.LayerNorm(hidden_dim // 4),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim // 4, hidden_dim // 8),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim // 8, 1),
            nn.Tanh()
        )
        
        # æƒé‡åˆ†é…ç½‘ç»œï¼šåªè¾“å‡º2ä¸ªåˆ†æ”¯æƒé‡
        self.weight_net_fc1 = nn.Linear(1, hidden_dim // 4)
        self.weight_net_fc2 = nn.Linear(hidden_dim // 4, hidden_dim // 8)
        self.weight_net_out = nn.Linear(hidden_dim // 8, 2)
        
        # èåˆå±‚ï¼ˆä½¿ç”¨æ›´å…ˆè¿›çš„Cross-Attentionï¼‰
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=num_classes, num_heads=num_heads, dropout=dropout_rate, batch_first=True
        )
        self.fusion_fc = nn.Sequential(
            nn.Linear(num_classes, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, num_classes)
        )
        
        self._initialize_weights()
    
    def _build_vision_transformer_path(self, input_dim, num_classes, dropout_rate):
        """æ„å»ºVision Transformeré£æ ¼çš„è·¯å¾„"""
        # è‡ªåŠ¨è®¡ç®—ä¸­é—´å±‚ç»´åº¦
        mid_dim = max(256, min(512, input_dim * 2))
        
        return nn.Sequential(
            # è¾“å…¥æŠ•å½±
            nn.Linear(input_dim, mid_dim),
            nn.LayerNorm(mid_dim),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            
            # å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚
            nn.TransformerEncoderLayer(
                d_model=mid_dim, nhead=8, dim_feedforward=mid_dim * 2,
                dropout=dropout_rate, activation='gelu', batch_first=True,
                norm_first=True  # Pre-LN for better stability
            ),
            
            # é™ç»´å’Œè¾“å‡º
            nn.Linear(mid_dim, mid_dim // 2),
            nn.LayerNorm(mid_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(mid_dim // 2, num_classes)
        )
    
    def _build_lightweight_path(self, input_dim, num_classes, dropout_rate):
        """æ„å»ºå¢å¼ºç‰ˆè½»é‡çº§æ³¨æ„åŠ›ç½‘ç»œè·¯å¾„"""
        class EnhancedAttention(nn.Module):
            def __init__(self, dim):
                super().__init__()
                self.norm1 = nn.LayerNorm(dim)
                self.norm2 = nn.LayerNorm(dim)
                
                # å¤šå¤´è‡ªæ³¨æ„åŠ›
                self.attention = nn.MultiheadAttention(
                    embed_dim=dim,
                    num_heads=4,
                    dropout=dropout_rate,
                    batch_first=True
                )
                
                # å‰é¦ˆç½‘ç»œ
                self.ffn = nn.Sequential(
                    nn.Linear(dim, dim * 2),
                    nn.GELU(),
                    nn.Dropout(dropout_rate),
                    nn.Linear(dim * 2, dim),
                    nn.Dropout(dropout_rate)
                )
            
            def forward(self, x):
                # è‡ªæ³¨æ„åŠ›
                attn_output, _ = self.attention(
                    self.norm1(x).unsqueeze(1),
                    self.norm1(x).unsqueeze(1),
                    self.norm1(x).unsqueeze(1)
                )
                x = x + attn_output.squeeze(1)
                
                # å‰é¦ˆç½‘ç»œ
                x = x + self.ffn(self.norm2(x))
                return x
        
        return nn.Sequential(
            # ç‰¹å¾æå–
            nn.Linear(input_dim, input_dim // 2),
            nn.LayerNorm(input_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            
            # å¢å¼ºç‰ˆæ³¨æ„åŠ›å±‚
            EnhancedAttention(input_dim // 2),
            
            # ç‰¹å¾å¢å¼º
            nn.Linear(input_dim // 2, input_dim // 4),
            nn.LayerNorm(input_dim // 4),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            
            # æ®‹å·®è¿æ¥
            nn.Sequential(
                nn.Linear(input_dim // 4, input_dim // 4),
                nn.LayerNorm(input_dim // 4),
                nn.GELU(),
                nn.Dropout(dropout_rate),
                nn.Linear(input_dim // 4, input_dim // 4)
            ),
            
            # è¾“å‡ºå±‚
            nn.Linear(input_dim // 4, num_classes)
        )
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        with torch.no_grad():
            nn.init.xavier_normal_(self.weight_net_fc1.weight)
            nn.init.xavier_normal_(self.weight_net_fc2.weight)
            nn.init.xavier_normal_(self.weight_net_out.weight)
            self.weight_net_out.bias.fill_(0.0)
    
    def _compute_adaptive_weights(self, snr):
        """è®¡ç®—è‡ªé€‚åº”æƒé‡"""
        if snr.dim() > 1 and snr.size(1) == 1:
            x = snr
        else:
            x = snr.unsqueeze(1) if snr.dim() == 1 else snr
        x = F.leaky_relu(self.weight_net_fc1(x))
        x = F.leaky_relu(self.weight_net_fc2(x))
        weights_logits = self.weight_net_out(x)
        weights = F.softmax(weights_logits, dim=1)
        return weights
    
    def forward(self, x):
        # ç‰¹å¾é¢„å¤„ç†
        processed_features = self.feature_processor(x)  # [batch_size, hidden_dim * 2]
        
        # åˆ†å‰²ç‰¹å¾ä¸ºç¬æ€å’Œç¨³æ€éƒ¨åˆ†
        mid_dim = processed_features.size(1) // 2
        trans_part = processed_features[:, :mid_dim]
        steady_part = processed_features[:, mid_dim:]
        
        # ç‰¹å¾æå–
        trans_features = self.transient_extractor(trans_part)
        steady_features = self.steady_extractor(steady_part)
        
        # èåˆç‰¹å¾
        combined = torch.cat([trans_features, steady_features], dim=1)
        features = self.fused_extractor(combined)
        
        # SNRä¼°è®¡
        snr_freq = self.snr_estimator_freq(features)
        snr_energy = self.snr_estimator_energy(features)
        snr_combined = torch.cat([snr_freq, snr_energy], dim=1)
        snr = self.snr_fusion(snr_combined) * self.snr_scale
        
        # è·å–ä¸¤ä¸ªå…ˆè¿›æ¨¡å‹çš„è¾“å‡º
        # Vision Transformerè·¯å¾„
        vit_input = features.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦ [batch, 1, dim]
        vit_features = vit_input
        # é€šè¿‡TransformerEncoderLayerï¼ˆå®ƒåœ¨Sequentialä¸­ï¼‰
        for layer in self.vision_transformer:
            if isinstance(layer, nn.TransformerEncoderLayer):
                vit_features = layer(vit_features)
            else:
                vit_features = layer(vit_features.squeeze(1) if vit_features.dim() > 2 else vit_features)
                if isinstance(layer, nn.Linear) and layer.out_features == self.num_classes:
                    break
                if vit_features.dim() == 2:
                    vit_features = vit_features.unsqueeze(1)
        
        vit_logits = vit_features.squeeze(1) if vit_features.dim() > 2 else vit_features
        
        # è½»é‡çº§ç½‘ç»œè·¯å¾„
        lw_logits = self.lightweight_network(features)
        
        # è®¡ç®—è‡ªé€‚åº”æƒé‡
        weights = self._compute_adaptive_weights(snr)
        
        # åŠ æƒèåˆ
        weighted_vit = vit_logits * weights[:, 0].unsqueeze(1)
        weighted_lw = lw_logits * weights[:, 1].unsqueeze(1)
        
        # Cross-Attentionèåˆï¼ˆæ›´å…ˆè¿›çš„èåˆæ–¹å¼ï¼‰
        # å‡†å¤‡è¾“å…¥ï¼š[batch, 2, num_classes]
        fusion_input = torch.stack([weighted_vit, weighted_lw], dim=1)
        
        # Cross-attention
        attn_output, _ = self.cross_attention(fusion_input, fusion_input, fusion_input)
        
        # æ± åŒ–å¹¶è¾“å‡º
        fusion_pooled = attn_output.mean(dim=1)  # [batch, num_classes]
        final_logits = self.fusion_fc(fusion_pooled)
        
        return final_logits, snr, weights

class ImprovedAdaptiveWeightedLoss(nn.Module):
    """æ”¹è¿›çš„è‡ªé€‚åº”åŠ æƒæŸå¤±å‡½æ•°ï¼ˆé€‚é…å…ˆè¿›ç½‘ç»œï¼‰"""
    def __init__(self, num_classes, snr_guidance_weight=0.05):
        super(ImprovedAdaptiveWeightedLoss, self).__init__()
        self.ce_loss = nn.CrossEntropyLoss(reduction='none', label_smoothing=0.05)
        self.num_classes = num_classes
        self.snr_guidance_weight = snr_guidance_weight
    
    def forward(self, final_logits, vit_logits, lw_logits, targets, snr, model_weights):
        vit_loss = self.ce_loss(vit_logits, targets).mean()
        lw_loss = self.ce_loss(lw_logits, targets).mean()
        ensemble_loss = self.ce_loss(final_logits, targets).mean()
        
        normalized_snr = torch.clamp(snr / 30.0, -1.0, 1.0)
        
        # å¯¹é«˜SNRå¼•å¯¼ViTåˆ†æ”¯ï¼ˆViTåœ¨é«˜SNRä¸‹é€šå¸¸è¡¨ç°æ›´å¥½ï¼‰
        high_snr_mask = (normalized_snr > 0.3).squeeze()
        high_snr_guidance = torch.zeros_like(ensemble_loss)
        if high_snr_mask.any():
            vit_weight_loss = F.relu(0.7 - model_weights[high_snr_mask, 0]).mean()
            high_snr_guidance = vit_weight_loss * self.snr_guidance_weight
        
        # å¯¹ä½SNRå¼•å¯¼è½»é‡çº§ç½‘ç»œåˆ†æ”¯ï¼ˆè½»é‡çº§ç½‘ç»œåœ¨å¤æ‚éçº¿æ€§åœºæ™¯ä¸‹è¡¨ç°æ›´å¥½ï¼‰
        low_snr_mask = (normalized_snr < -0.3).squeeze()
        low_snr_guidance = torch.zeros_like(ensemble_loss)
        if low_snr_mask.any():
            lw_weight_loss = F.relu(0.7 - model_weights[low_snr_mask, 1]).mean()
            low_snr_guidance = lw_weight_loss * self.snr_guidance_weight
        
        total_loss = ensemble_loss + 0.1 * (vit_loss + lw_loss) + high_snr_guidance + low_snr_guidance
        
        component_losses = {
            'vit_loss': vit_loss.item(),
            'lw_loss': lw_loss.item(),
            'ensemble_loss': ensemble_loss.item(),
            'weighted_loss': total_loss.item()
        }
        
        return total_loss, component_losses

class NoiseDataLoader:
    """æ·»åŠ å™ªå£°çš„æ•°æ®åŠ è½½å™¨ï¼ˆå®Œå…¨å‚è€ƒbuild_networkç­–ç•¥ï¼‰"""
    def __init__(self, dataset, batch_size=32, shuffle=True, 
                noise_prob=0.5, min_snr=-10, max_snr=20, 
                snr_high=0.90, snr_mid=0.08, snr_low=0.02):
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.noise_prob = noise_prob
        self.min_snr = min_snr
        self.max_snr = max_snr
        self.snr_high = snr_high
        self.snr_mid = snr_mid
        self.snr_low = snr_low
        
        # åˆ›å»ºåŸå§‹æ•°æ®åŠ è½½å™¨
        self.dataloader = DataLoader(
            dataset, 
            batch_size=batch_size, 
            shuffle=shuffle
        )
    
    def __iter__(self):
        self.iterator = iter(self.dataloader)
        return self
    
    def __next__(self):
        # è·å–ä¸‹ä¸€æ‰¹æ•°æ®
        batch_x, batch_y = next(self.iterator)
        
        # ä¸ºæ¯ä¸ªæ ·æœ¬ç”ŸæˆéšæœºSNRå€¼
        batch_size = batch_x.size(0)
        batch_snr = torch.zeros(batch_size, 1)
        
        # æœ‰ä¸€å®šæ¦‚ç‡æ·»åŠ å™ªå£°
        if random.random() < self.noise_prob:
            batch_x, actual_snrs = add_advanced_noise_to_batch(
                batch_x, 
                min_snr=self.min_snr, max_snr=self.max_snr,
                snr_high=self.snr_high, snr_mid=self.snr_mid, snr_low=self.snr_low
            )
            batch_snr = torch.tensor(actual_snrs).float().unsqueeze(1)
        else:
            # å¦‚æœä¸æ·»åŠ å™ªå£°ï¼Œå‡è®¾SNRä¸ºæœ€å¤§å€¼
            batch_snr.fill_(self.max_snr)
        
        return batch_x, batch_y, batch_snr
    
    def __len__(self):
        return len(self.dataloader)

def train_build_inspired_aafn(model, train_dataset, val_dataset, num_epochs=40, 
                            batch_size=None, lr=None, save_dir='./build_inspired_results'):
    """è®­ç»ƒbuild_networké£æ ¼çš„AAFNæ¨¡å‹"""
    
    os.makedirs(save_dir, exist_ok=True)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    model = model.to(device)
    
    # è‡ªåŠ¨è®¡ç®—æ‰¹æ¬¡å¤§å°
    if batch_size is None:
        batch_size = min(32, max(16, len(train_dataset) // 100))  # å‡å°æ‰¹æ¬¡å¤§å°
    
    # è‡ªåŠ¨è®¡ç®—å­¦ä¹ ç‡
    if lr is None:
        lr = 0.0005 * (batch_size / 32) ** 0.5  # é™ä½åŸºç¡€å­¦ä¹ ç‡
    
    # è‡ªåŠ¨è®¡ç®—æ—©åœè€å¿ƒå€¼
    early_stop_patience = max(10, min(20, num_epochs // 3))
    
    # è‡ªåŠ¨è®¡ç®—æœ€å°å­¦ä¹ ç‡æ¯”ä¾‹
    min_lr_ratio = max(0.01, min(0.1, 1.0 / (num_epochs ** 0.5)))
    
    print(f"è‡ªåŠ¨é€‚é…è®­ç»ƒå‚æ•°:")
    print(f"- æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"- å­¦ä¹ ç‡: {lr:.6f}")
    print(f"- æ—©åœè€å¿ƒå€¼: {early_stop_patience}")
    print(f"- æœ€å°å­¦ä¹ ç‡æ¯”ä¾‹: {min_lr_ratio:.3f}")
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = ImprovedAdaptiveWeightedLoss(num_classes=model.num_classes)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=2e-4)  # å¢åŠ æƒé‡è¡°å‡
    
    # ä½¿ç”¨å¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«è°ƒåº¦å™¨
    warmup_epochs = 5
    total_steps = len(train_dataset) // batch_size * num_epochs
    warmup_steps = len(train_dataset) // batch_size * warmup_epochs
    
    def lr_lambda(step):
        if step < warmup_steps:
            return float(step) / float(max(1, warmup_steps))
        progress = float(step - warmup_steps) / float(total_steps - warmup_steps)
        return 0.5 * (1 + math.cos(math.pi * progress))
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # åˆ›å»ºå¸¦å™ªå£°çš„æ•°æ®åŠ è½½å™¨
    train_loader = NoiseDataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        noise_prob=0.5,  # é™ä½å™ªå£°æ¦‚ç‡
        min_snr=-10,     # è°ƒæ•´SNRèŒƒå›´
        max_snr=20,
        snr_high=0.90,   # å¢åŠ é«˜SNRæ ·æœ¬æ¯”ä¾‹
        snr_mid=0.08,
        snr_low=0.02
    )
    
    # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': []
    }
    
    # æœ€ä½³æ¨¡å‹è¿½è¸ª
    best_val_acc = 0.0
    best_val_loss = float('inf')  # æ·»åŠ æœ€ä½³éªŒè¯æŸå¤±è¿½è¸ª
    patience = early_stop_patience
    early_stop_counter = 0
    
    print(f"å¼€å§‹è®­ç»ƒbuild_networké£æ ¼çš„AAFNæ¨¡å‹...")
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
        for batch_x, batch_y, batch_snr in pbar:
            batch_x, batch_y, batch_snr = (
                batch_x.to(device), 
                batch_y.to(device),
                batch_snr.to(device)
            )
            
            # æ¸…é›¶æ¢¯åº¦
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            final_logits, snr, model_weights = model(batch_x)
            
            # è·å–å„åˆ†æ”¯logits
            with torch.no_grad():
                # é‡æ–°è®¡ç®—ç‰¹å¾
                processed_features = model.feature_processor(batch_x)
                mid_dim = processed_features.size(1) // 2
                trans_part = processed_features[:, :mid_dim]
                steady_part = processed_features[:, mid_dim:]
                trans_features = model.transient_extractor(trans_part)
                steady_features = model.steady_extractor(steady_part)
                combined = torch.cat([trans_features, steady_features], dim=1)
                features = model.fused_extractor(combined)
                
                # ViTåˆ†æ”¯è¾“å‡º
                vit_input = features.unsqueeze(1)
                vit_features = vit_input
                for layer in model.vision_transformer:
                    if isinstance(layer, nn.TransformerEncoderLayer):
                        vit_features = layer(vit_features)
                    else:
                        vit_features = layer(vit_features.squeeze(1) if vit_features.dim() > 2 else vit_features)
                        if isinstance(layer, nn.Linear) and layer.out_features == model.num_classes:
                            break
                        if vit_features.dim() == 2:
                            vit_features = vit_features.unsqueeze(1)
                vit_logits = vit_features.squeeze(1) if vit_features.dim() > 2 else vit_features
                
                # è½»é‡çº§ç½‘ç»œè¾“å‡º
                lw_logits = model.lightweight_network(features)
            
            # æŸå¤±å‡½æ•°è°ƒç”¨
            loss, component_losses = criterion(final_logits, vit_logits, lw_logits, batch_y, snr, model_weights)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # å‡å°æ¢¯åº¦è£å‰ªé˜ˆå€¼
            
            # ä¼˜åŒ–æ­¥éª¤
            optimizer.step()
            scheduler.step()
            
            # ç»Ÿè®¡
            train_loss += loss.item() * batch_y.size(0)
            _, predicted = torch.max(final_logits, 1)
            train_total += batch_y.size(0)
            train_correct += (predicted == batch_y).sum().item()
            
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{train_correct/train_total:.4f}'
            })
        
        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡
        train_loss = train_loss / train_total
        train_accuracy = train_correct / train_total
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                
                # å‰å‘ä¼ æ’­
                final_logits, snr, model_weights = model(batch_x)
                # è·å–å„åˆ†æ”¯logits
                processed_features = model.feature_processor(batch_x)
                mid_dim = processed_features.size(1) // 2
                trans_part = processed_features[:, :mid_dim]
                steady_part = processed_features[:, mid_dim:]
                trans_features = model.transient_extractor(trans_part)
                steady_features = model.steady_extractor(steady_part)
                combined = torch.cat([trans_features, steady_features], dim=1)
                features = model.fused_extractor(combined)
                
                # ViTåˆ†æ”¯è¾“å‡º
                vit_input = features.unsqueeze(1)
                vit_features = vit_input
                for layer in model.vision_transformer:
                    if isinstance(layer, nn.TransformerEncoderLayer):
                        vit_features = layer(vit_features)
                    else:
                        vit_features = layer(vit_features.squeeze(1) if vit_features.dim() > 2 else vit_features)
                        if isinstance(layer, nn.Linear) and layer.out_features == model.num_classes:
                            break
                        if vit_features.dim() == 2:
                            vit_features = vit_features.unsqueeze(1)
                vit_logits = vit_features.squeeze(1) if vit_features.dim() > 2 else vit_features
                
                # è½»é‡çº§ç½‘ç»œè¾“å‡º
                lw_logits = model.lightweight_network(features)
                
                # æŸå¤±å‡½æ•°è°ƒç”¨
                loss, _ = criterion(final_logits, vit_logits, lw_logits, batch_y, snr, model_weights)
                
                # ç»Ÿè®¡
                val_loss += loss.item() * batch_y.size(0)
                _, predicted = torch.max(final_logits, 1)
                val_total += batch_y.size(0)
                val_correct += (predicted == batch_y).sum().item()
        
        # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±å’Œå‡†ç¡®ç‡
        val_loss = val_loss / val_total
        val_accuracy = val_correct / val_total
        
        # ä¿å­˜è®­ç»ƒå†å²
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_accuracy)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_accuracy)
        
        print(f"Epoch {epoch+1}/{num_epochs} - "
              f"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŒæ—¶è€ƒè™‘å‡†ç¡®ç‡å’ŒæŸå¤±ï¼‰
        if val_accuracy > best_val_acc or (val_accuracy == best_val_acc and val_loss < best_val_loss):
            best_val_acc = val_accuracy
            best_val_loss = val_loss
            torch.save(model.state_dict(), os.path.join(save_dir, 'best_build_inspired_aafn.pth'))
            print(f"ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}, éªŒè¯æŸå¤±: {val_loss:.4f}")
            early_stop_counter = 0
        else:
            early_stop_counter += 1
            if early_stop_counter >= patience:
                print(f"æ—©åœè§¦å‘ï¼éªŒè¯é›†æ€§èƒ½è¿ç»­{patience}æ¬¡æœªæå‡ï¼Œæå‰ç»ˆæ­¢è®­ç»ƒã€‚")
                break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load(os.path.join(save_dir, 'best_build_inspired_aafn.pth')))
    
    return model, best_val_acc, history

def test_snr_performance(model, test_dataset, batch_size=64, device='cuda'):
    """æµ‹è¯•ä¸åŒSNRä¸‹çš„æ€§èƒ½"""
    
    snr_range = [20, 15, 10, 5, 0, -5, -10]
    
    model.eval()
    results = {'snr_db': snr_range, 'accuracy': [], 'weights': []}
    
    print("ğŸ¯ æµ‹è¯•build_networké£æ ¼AAFNæ€§èƒ½...")
    
    for snr_db in snr_range:
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        correct = 0
        total = 0
        all_weights = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
                if batch_x.dim() == 1:
                    batch_x = batch_x.unsqueeze(0)
                if batch_y.dim() == 0:
                    batch_y = batch_y.unsqueeze(0)
                
                # åªåœ¨æµ‹è¯•æ—¶æ·»åŠ æŒ‡å®šSNRçš„å™ªå£°
                if snr_db < 20:
                    batch_x, _ = add_advanced_noise_to_batch(
                        batch_x, min_snr=snr_db, max_snr=snr_db
                    )
                
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                
                logits, _, weights = model(batch_x)
                _, predicted = torch.max(logits, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
                
                all_weights.append(weights.cpu().numpy())
        
        accuracy = correct / total
        avg_weights = np.mean(np.concatenate(all_weights), axis=0)
        
        results['accuracy'].append(accuracy)
        results['weights'].append(avg_weights.tolist())
        
        print(f"SNR {snr_db:3d} dB: {accuracy:.4f}")
        print(f"   æƒé‡ - ViT: {avg_weights[0]:.3f}, è½»é‡çº§ç½‘ç»œ: {avg_weights[1]:.3f}")
    
    return results

def extract_features_for_visualization(model, dataset, device, max_samples=2000):
    model.eval()
    features_list = []
    all_labels = []
    dataloader = DataLoader(dataset, batch_size=64, shuffle=False)
    sample_count = 0
    with torch.no_grad():
        for batch_x, batch_y in dataloader:
            if sample_count >= max_samples:
                break
            batch_x = batch_x.to(device)
            _ = model(batch_x)
            features_list.append(batch_x.cpu())  # è¿™é‡Œå‡è®¾ç›´æ¥ç”¨è¾“å…¥ç‰¹å¾ï¼Œå¦‚éœ€ä¸­é—´å±‚å¯æ”¹
            all_labels.extend(batch_y.cpu().numpy())
            sample_count += len(batch_y)
    features = torch.cat(features_list, dim=0)[:max_samples]
    labels = np.array(all_labels)[:max_samples]
    return features.numpy(), labels

def create_individual_identification_plot(model, test_dataset, save_dir, device, max_samples=2000):
    print("ğŸ¨ ç”Ÿæˆä¸ªä½“è¯†åˆ«å¯è§†åŒ–å›¾ (t-SNE)...")
    features, labels = extract_features_for_visualization(model, test_dataset, device, max_samples)
    if features is None:
        print("âŒ ç‰¹å¾æå–å¤±è´¥")
        return
    print(f"ç‰¹å¾shape: {features.shape}, æ ‡ç­¾æ•°é‡: {len(np.unique(labels))}")
    reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)//4))
    features_2d = reducer.fit_transform(features)
    unique_labels = np.unique(labels)
    n_classes = len(unique_labels)
    import matplotlib.pyplot as plt
    if n_classes <= 10:
        colors = plt.cm.tab10(np.linspace(0, 1, n_classes))
    elif n_classes <= 20:
        colors = plt.cm.tab20(np.linspace(0, 1, n_classes))
    else:
        colors = plt.cm.nipy_spectral(np.linspace(0, 1, n_classes))
    plt.figure(figsize=(14, 12))
    for i, label in enumerate(unique_labels):
        mask = labels == label
        plt.scatter(features_2d[mask, 0], features_2d[mask, 1], c=[colors[i]], label=f'Aircraft {label:03d}', s=30, alpha=0.7, edgecolors='black', linewidth=0.5)
    plt.title('Individual Aircraft Identification Visualization (TSNE)', fontsize=16, fontweight='bold')
    plt.xlabel('TSNE Component 1', fontsize=12)
    plt.ylabel('TSNE Component 2', fontsize=12)
    if n_classes <= 30:
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8, ncol=max(1, n_classes//20))
    else:
        sm = plt.cm.ScalarMappable(cmap=plt.cm.nipy_spectral, norm=plt.Normalize(vmin=unique_labels.min(), vmax=unique_labels.max()))
        sm.set_array([])
        cbar = plt.colorbar(sm)
        cbar.set_label('Aircraft ID', rotation=270, labelpad=15)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, 'individual_identification_tsne.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ä¸ªä½“è¯†åˆ«å›¾å·²ä¿å­˜: {save_path}")
    return features_2d, labels

def save_snr_performance_plot_and_csv(results, save_dir):
    import matplotlib.pyplot as plt
    import pandas as pd
    os.makedirs(save_dir, exist_ok=True)
    # ä¿å­˜csv
    df = pd.DataFrame({'SNR (dB)': results['snr_db'], 'Accuracy': results['accuracy']})
    csv_path = os.path.join(save_dir, 'final_snr_performance.csv')
    df.to_csv(csv_path, index=False)
    # ç»˜å›¾
    plt.figure(figsize=(7,5))
    plt.plot(results['snr_db'], results['accuracy'], 'o-', linewidth=2)
    plt.xlabel('SNR (dB)')
    plt.ylabel('Accuracy')
    plt.title('Model Accuracy vs SNR (Grouped Features)')
    plt.grid(True)
    plt.ylim(0, 1.05)
    plt.tight_layout()
    png_path = os.path.join(save_dir, 'final_snr_performance.png')
    plt.savefig(png_path, dpi=300)
    plt.close()
    print(f"SNRæ€§èƒ½æ›²çº¿å’ŒCSVå·²ä¿å­˜: {png_path}, {csv_path}")

def save_training_history_plot(history, save_dir):
    import matplotlib.pyplot as plt
    os.makedirs(save_dir, exist_ok=True)
    plt.figure(figsize=(15, 10))
    # æŸå¤±æ›²çº¿
    plt.subplot(2, 2, 1)
    epochs = range(1, len(history['train_loss']) + 1)
    plt.plot(epochs, history['train_loss'], 'b-', label='training loss')
    plt.plot(epochs, history['val_loss'], 'r-', label='validation loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    # å‡†ç¡®ç‡æ›²çº¿
    plt.subplot(2, 2, 2)
    plt.plot(epochs, history['train_acc'], 'b-', label='training accuracy')
    plt.plot(epochs, history['val_acc'], 'r-', label='validation accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    png_path = os.path.join(save_dir, 'improved_training_history.png')
    plt.savefig(png_path, dpi=300)
    plt.close()
    print(f"è®­ç»ƒå†å²æ›²çº¿å·²ä¿å­˜: {png_path}")

def create_confusion_matrix(model, test_dataset, save_dir, device):
    """ç”Ÿæˆå¹¶ä¿å­˜æ··æ·†çŸ©é˜µ"""
    print("ğŸ¯ ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
    model.eval()
    all_predictions = []
    all_labels = []
    
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            logits, _, _ = model(batch_x)
            _, predicted = torch.max(logits, 1)
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(batch_y.numpy())
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    from sklearn.metrics import confusion_matrix
    import seaborn as sns
    
    cm = confusion_matrix(all_labels, all_predictions)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(20, 16))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=range(len(np.unique(all_labels))),
                yticklabels=range(len(np.unique(all_labels))))
    plt.title('Confusion Matrix for Individual Aircraft Identification', fontsize=16)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    
    # ä¿å­˜æ··æ·†çŸ©é˜µå›¾
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, 'confusion_matrix.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
    
    # è®¡ç®—å¹¶æ‰“å°ä¸€äº›è¯„ä¼°æŒ‡æ ‡
    from sklearn.metrics import classification_report
    report = classification_report(all_labels, all_predictions, output_dict=True)
    
    # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    import json
    report_path = os.path.join(save_dir, 'classification_report.json')
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"âœ… åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    return cm, report

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è®­ç»ƒbuild_networké£æ ¼çš„AAFNæ¨¡å‹")
    parser.add_argument('--data', type=str, required=True, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=40, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=None, help='æ‰¹æ¬¡å¤§å°ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ï¼‰')
    parser.add_argument('--lr', type=float, default=None, help='å­¦ä¹ ç‡ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ï¼‰')
    parser.add_argument('--save_dir', type=str, default='./build_inspired_aafn_results', help='ä¿å­˜ç›®å½•')
    parser.add_argument('--hidden_dim', type=int, default=None, help='éšè—å±‚ç»´åº¦ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ï¼‰')
    
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    train_dataset, val_dataset, test_dataset, num_classes, label_map, scaler = load_and_preprocess_data(args.data)
    
    sample_x, _ = train_dataset[0]
    input_dim = sample_x.size(0)
    
    print(f"è¾“å…¥ç»´åº¦: {input_dim}, ç±»åˆ«æ•°é‡: {num_classes}")
    
    # åˆ›å»ºbuild_networké£æ ¼çš„æ¨¡å‹
    model = BuildInspiredAAFN(
        input_dim=input_dim,
        num_classes=num_classes,
        hidden_dim=args.hidden_dim
    )
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")
    
    # è®­ç»ƒæ¨¡å‹
    model, best_val_acc, history = train_build_inspired_aafn(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        lr=args.lr,
        save_dir=args.save_dir
    )
    
    # æµ‹è¯•æ€§èƒ½
    results = test_snr_performance(model, test_dataset, batch_size=args.batch_size, device=device)
    
    # ä¿å­˜SNRæ€§èƒ½æ›²çº¿å’ŒCSV
    save_snr_performance_plot_and_csv(results, args.save_dir)
    # ä¿å­˜è®­ç»ƒå†å²æ›²çº¿
    save_training_history_plot(history, args.save_dir)
    # ä¿å­˜t-SNEå¯è§†åŒ–
    create_individual_identification_plot(model, test_dataset, args.save_dir, device)
    # ç”Ÿæˆå¹¶ä¿å­˜æ··æ·†çŸ©é˜µ
    create_confusion_matrix(model, test_dataset, args.save_dir, device)
    
    # ç»˜åˆ¶ç»“æœå¯¹æ¯”å›¾
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 1, 1)
    plt.plot(results['snr_db'], results['accuracy'], 'ro-', linewidth=3, markersize=8, label='Build-style AAFN')
    plt.axhline(y=0.95, color='g', linestyle='--', linewidth=2, label='95% Target Line')
    plt.xlabel('SNR (dB)')
    plt.ylabel('Accuracy')
    plt.title('Build-style AAFN Performance')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.ylim(0.5, 1.05)
    
    plt.subplot(2, 1, 2)
    weights_array = np.array(results['weights'])
    plt.plot(results['snr_db'], weights_array[:, 0], 'ro-', label='ViT Weight', linewidth=2)
    plt.plot(results['snr_db'], weights_array[:, 1], 'bo-', label='Lightweight Network Weight', linewidth=2)
    plt.xlabel('SNR (dB)')
    plt.ylabel('Weight')
    plt.title('Advanced Network Branch Weights Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(args.save_dir, 'build_inspired_aafn_performance.png'), dpi=300)
    plt.close()
    
    # ä¿å­˜ç»“æœ
    import json
    with open(os.path.join(args.save_dir, 'build_inspired_results.json'), 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {args.save_dir}")

if __name__ == "__main__":
    main()